
use crate::compiler::source_holder::SourceHolder;
use crate::compiler::symbol::SymbolFactory;
use crate::tokenizer::tokenize::Tokenizer;
use crate::tokenizer::tokens::{Comment, Delimiter, Keyword, Literal, Operator, Token};

/// ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚
/// æ–‡å­—åˆ—ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’è¿”ã™ã€‚
fn tokenize_helper(input: &str) -> Vec<Token> {
    let source_holder = SourceHolder::new(input);
    let mut symbol_factory = SymbolFactory::new(source_holder);
    let tokenizer = Tokenizer::new(input, &mut symbol_factory);
    tokenizer.tokenize().expect("Tokenization failed")
}

#[test]
fn test_basic_keywords() {
    let input = "let mut pub fn class if else match return";
    let tokens = tokenize_helper(input);

    assert_eq!(tokens[0], Token::Keyword(Keyword::Let));
    assert_eq!(tokens[1], Token::Keyword(Keyword::Mut));
    assert_eq!(tokens[2], Token::Keyword(Keyword::Pub));
    assert_eq!(tokens[3], Token::Keyword(Keyword::Fn));
    assert_eq!(tokens[4], Token::Keyword(Keyword::Class));
    assert_eq!(tokens[5], Token::Keyword(Keyword::If));
    assert_eq!(tokens[6], Token::Keyword(Keyword::Else));
    assert_eq!(tokens[7], Token::Keyword(Keyword::Match));
    assert_eq!(tokens[8], Token::Keyword(Keyword::Return));
    assert_eq!(tokens[9], Token::EndOfFile);
}

#[test]
fn test_numbers() {
    // æ•´æ•°ã€æµ®å‹•å°æ•°ç‚¹ã€16é€²æ•°ã€2é€²æ•°ã€ç§‘å­¦è¡¨è¨˜
    let input = "123 3.14 0xFF 0b1010 1.2e+10";
    let tokens = tokenize_helper(input);

    assert!(matches!(
        tokens[0],
        Token::Literal(Literal::IntegerLiteral(123))
    ));
    // f32ã®æ¯”è¼ƒã¯HashableFloatã‚’ä»‹ã—ã¦è¡Œã‚ã‚Œã‚‹
    if let Token::Literal(Literal::FloatLiteral(f)) = tokens[1] {
        assert_eq!(f.get(), 3.14);
    } else {
        panic!("Expected float");
    }

    assert!(matches!(
        tokens[2],
        Token::Literal(Literal::IntegerLiteral(255))
    )); // 0xFF
    assert!(matches!(
        tokens[3],
        Token::Literal(Literal::IntegerLiteral(10))
    )); // 0b1010

    if let Token::Literal(Literal::FloatLiteral(f)) = tokens[4] {
        assert_eq!(f.get(), 1.2e10);
    } else {
        panic!("Expected float scientific");
    }
}

#[test]
fn test_operators_and_ranges() {
    // æœ€å¤§é•·ã®ã‚‚ã®ã‹ã‚‰å„ªå…ˆçš„ã«ãƒãƒƒãƒã™ã‚‹ã‹(..= vs .. vs .)
    let input = ".. ..= . => -> |>";
    let tokens = tokenize_helper(input);

    assert_eq!(tokens[0], Token::Operator(Operator::RangeExclusive));
    assert_eq!(tokens[1], Token::Operator(Operator::RangeInclusive));
    assert_eq!(tokens[2], Token::Operator(Operator::MemberAccess));
    assert_eq!(tokens[3], Token::Operator(Operator::FatArrow));
    assert_eq!(tokens[4], Token::Operator(Operator::Arrow));
    assert_eq!(tokens[5], Token::Operator(Operator::Pipe));
}

#[test]
fn test_comments() {
    let input = "// line comment\n/// doc comment\n/* block */ /* nested /* block */ */";
    let tokens = tokenize_helper(input);

    assert_eq!(tokens[0], Token::Comment(Comment::LineComment));
    assert!(matches!(tokens[1], Token::Comment(Comment::DocComment(_))));
    assert_eq!(tokens[2], Token::Comment(Comment::BlockComment));
    assert_eq!(tokens[3], Token::Comment(Comment::BlockComment)); // ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚‚ã®ã‚‚1ã¤ã®BlockCommentã¨ã—ã¦å‡¦ç†
}

#[test]
fn test_multibyte_safety_in_comments_and_strings() {
    // æ—¥æœ¬èªï¼ˆ3ãƒã‚¤ãƒˆæ–‡å­—ï¼‰ã‚„çµµæ–‡å­—ï¼ˆ4ãƒã‚¤ãƒˆæ–‡å­—ï¼‰ã‚’å«ã‚€ã‚±ãƒ¼ã‚¹
    // Tokenizerã®å„ãƒ¡ã‚½ãƒƒãƒ‰ãŒãƒã‚¤ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã¯ãªãæ–‡å­—å¢ƒç•Œã‚’æ„è­˜ã§ãã¦ã„ã‚‹ã‹ç¢ºèª
    let input = r#"
            // æ—¥æœ¬èªã®ã‚³ãƒ¡ãƒ³ãƒˆ
            let s = "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œ ğŸŒ"; 
            /* ãƒãƒ«ãƒãƒã‚¤ãƒˆ
               ãƒ–ãƒ­ãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ */
            let c = 'ã‚';
        "#;

    // tokenizeä¸­ã«ãƒ‘ãƒ‹ãƒƒã‚¯ï¼ˆå¢ƒç•Œå¤–ã‚¢ã‚¯ã‚»ã‚¹ã‚„ä¸æ­£ãªUTF-8ã‚¹ãƒ©ã‚¤ã‚¹ä½œæˆï¼‰ãŒèµ·ããªã„ã“ã¨ã‚’ç¢ºèª
    let tokens = tokenize_helper(input);

    assert_eq!(tokens[0], Token::Comment(Comment::LineComment));
    assert_eq!(tokens[1], Token::Keyword(Keyword::Let));
    assert!(tokens[2].is_identifier());
    assert_eq!(tokens[3], Token::Operator(Operator::Assignment));

    // æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã®ä¸­èº«ãŒæ­£ã—ãSpanã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹
    if let Token::Literal(Literal::StringLiteral(span)) = tokens[4] {
        // æ–‡å­—åˆ—è‡ªä½“ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯Parser/Evaluatorã®è²¬å‹™ã ãŒã€
        // çµ‚ç«¯ã®å¼•ç”¨ç¬¦ãŒæ­£ã—ãèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹ãŒé‡è¦
        assert_eq!(tokens[5], Token::Delimiter(Delimiter::Semicolon));
    } else {
        panic!("Expected string literal");
    }

    assert_eq!(tokens[6], Token::Comment(Comment::BlockComment));
    assert_eq!(tokens[11], Token::Literal(Literal::CharLiteral('ã‚')));
}

#[test]
fn test_multibyte_error_handling() {
    // è­˜åˆ¥å­ã¨ã—ã¦è¨±å¯ã•ã‚Œã¦ã„ãªã„ãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—ãŒç›´æ¥ç¾ã‚ŒãŸå ´åˆ
    let input = "let ğŸ• = 1;";
    let source_holder = SourceHolder::new(input);
    let mut symbol_factory = SymbolFactory::new(source_holder);
    let tokenizer = Tokenizer::new(input, &mut symbol_factory);

    let result = tokenizer.tokenize();
    // ç¾åœ¨ã®å®Ÿè£…ã§ã¯ UnknownToken ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã¯ãš
    assert!(result.is_err());
}

#[test]
fn test_number_literal_after_multibyte() {
    // read_number_literal å†…ã® unsafe { std::str::from_utf8_unchecked }
    // ãŒç›´å‰ã®ãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—ã®å½±éŸ¿ã§ä¸æ­£ãªãƒã‚¤ãƒ³ã‚¿ã‚’å‚ç…§ã—ãªã„ã‹
    let input = "// ã‚\n123";
    let tokens = tokenize_helper(input);

    // ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆãƒãƒ«ãƒãƒã‚¤ãƒˆå«ã‚€ï¼‰ã®å¾Œã®æ•°å€¤ãŒæ­£ã—ãèª­ã¿å–ã‚Œã‚‹ã‹
    assert_eq!(tokens[0], Token::Comment(Comment::LineComment));
    assert!(matches!(
        tokens[1],
        Token::Literal(Literal::IntegerLiteral(123))
    ));
}

#[test]
fn test_string_escape_sequences() {
    let input = r#""line 1\nline 2\"quoted\"""#;
    let tokens = tokenize_helper(input);

    if let Token::Literal(Literal::StringLiteral(_)) = tokens[0] {
        // æ­£å¸¸ã«ã‚¯ãƒ­ãƒ¼ã‚ºã•ã‚Œã¦ã„ã‚‹
        assert_eq!(tokens[1], Token::EndOfFile);
    } else {
        panic!("String literal with escapes failed");
    }
}

#[test]
fn test_incomplete_tokens() {
    let source_holder = SourceHolder::new("\"unclosed string");
    let mut symbol_factory = SymbolFactory::new(source_holder);

    // é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«
    let tokenizer1 = Tokenizer::new("\"unclosed string", &mut symbol_factory);
    assert!(tokenizer1.tokenize().is_err());

    // é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„ãƒ–ãƒ­ãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ
    let tokenizer2 = Tokenizer::new("/* unclosed comment", &mut symbol_factory);
    assert!(tokenizer2.tokenize().is_err());

    // é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„æ–‡å­—ãƒªãƒ†ãƒ©ãƒ«
    let tokenizer3 = Tokenizer::new("'a", &mut symbol_factory);
    assert!(tokenizer3.tokenize().is_err());
}

#[test]
fn test_complex_scientific_notation() {
    let input = "1.0e+10 1.0E-10 1e5";
    let tokens = tokenize_helper(input);

    for i in 0..3 {
        if let Token::Literal(Literal::FloatLiteral(_)) = tokens[i] {
            // ok
        } else {
            panic!("Scientific notation failed at index {}", i);
        }
    }
}
